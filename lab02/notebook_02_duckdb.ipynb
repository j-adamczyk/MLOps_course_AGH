{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DuckDB and file formats",
   "id": "396294ac6ba1cd80"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This tutorial is slightly based on the DuckDB [tutorial](https://duckdb.org/2024/05/31/analyzing-railway-traffic-in-the-netherlands.html).",
   "id": "642797aeee7dd3c7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reading from files",
   "id": "fd3ba288e75e8165"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The only thing needed to run DuckDB is importing it. To compare with Postgres, we will read the CSV file with train services in 2024.\n",
    "\n",
    "By the way, it's highly recommended to always use multi-line strings in triple quotes for all queries, even the simplest ones. It makes them more readable, easier to modify, and removes escaping quotes."
   ],
   "id": "e744a6f80f55e1e3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import duckdb\n",
    "\n",
    "duckdb.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM \"data/services-2024.csv\"\n",
    "\"\"\")"
   ],
   "id": "1654b6a21c75fa78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now that was fast and easy! DuckDB parses CSV and infers type with [quite advanced features](https://duckdb.org/docs/stable/data/csv/auto_detection.html). You can also specify options manually ([documentation](https://duckdb.org/docs/stable/data/csv/overview.html)), e.g.:\n",
    "\n",
    "`FROM read_csv(\"flights.csv\", delim = \"|\")`.\n",
    "\n",
    "Jupyter Notebook automatically prints the results, but we could also save them and call `.show()` explicitly. It is returned as `DuckDBPyRelation` object ([documentation](https://duckdb.org/docs/stable/clients/python/reference/#duckdb.DuckDBPyRelation)). You can use the object-oriented Relational API ([documentation](https://duckdb.org/docs/stable/clients/python/relational_api.html)) instead of writing SQL to compose queries based on Python language constructs. It's less popular than SQL though, and can be less readable."
   ],
   "id": "30de8b965ab70543"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "results = duckdb.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM \"data/services-2024.csv\"\n",
    "\"\"\")\n",
    "print(type(results))\n",
    "results.show()"
   ],
   "id": "bfabb3ee9fe0814f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# last 10 rows\n",
    "duckdb.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM \"data/services-2024.csv\"\n",
    "ORDER BY \"Service:Date\"\n",
    "LIMIT 10\n",
    "\"\"\").show()"
   ],
   "id": "b66f679a641b51f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# total number of late trains\n",
    "duckdb.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM \"data/services-2024.csv\"\n",
    "WHERE \"Stop:Arrival delay\" > 0\n",
    "\"\"\")"
   ],
   "id": "f09f3b8e2ab96a21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Queries like above are transient (in-memory). They can temporarily spill data to disk to process larger-than-memory datasets, but don't save results. In particular, each query needs to parse CSV file.\n",
    "\n",
    "To make results persistent, we can create `.db` (or `.duckdb`) file, similarly to SQLite. This is useful for a few reasons:\n",
    "- you want to save work and go home\n",
    "- for storing intermediate computation results for longer workflows\n",
    "- avoiding reading and parsing files, or downloading remote data\n",
    "\n",
    "Such file holds regular tables, which can be created from other data sources."
   ],
   "id": "3b9ce2d740762429"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "db = duckdb.connect(\"data/duckdb_trains.db\")\n",
    "db"
   ],
   "id": "84b6c2a1326a79b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If you call methods on `db` object, they will be run on the database. If you use `duckdb` module, the default in-memory session will be used instead.\n",
    "\n",
    "Let's create table from CSV and compare sizes with on-disk file."
   ],
   "id": "44c754c48473de75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "db.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS services AS\n",
    "FROM \"data/services-2024.csv\"\n",
    "\"\"\")\n",
    "\n",
    "csv_size_mb = os.path.getsize(\"data/services-2024.csv\") // (1024 * 1024)\n",
    "duckdb_size_mb = os.path.getsize(\"data/duckdb_trains.db\") // (1024 * 1024)\n",
    "\n",
    "print(f\"CSV size: {csv_size_mb} MB\")\n",
    "print(f\"DuckDB size: {duckdb_size_mb} MB\")"
   ],
   "id": "fbe9e5fe01380161"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "DuckDB uses many [\"friendly SQL\"](https://duckdb.org/docs/stable/sql/dialect/friendly_sql.html) extensions, which make SQL queries easier to write. They also offer many built-in features and functions. Some of them are DuckDB-exclusive, while others (e.g. PIVOT, UNPIVOT) are commonly adopted by many SQL databases.\n",
    "\n",
    "Above, we use [FROM-first syntax](https://duckdb.org/docs/stable/sql/query_syntax/from.html#from-first-syntax), which allows omitting `SELECT *`. Without it, the query would be `AS SELECT * FROM ...`.\n",
    "\n",
    "It also allows switching order of SELECT and FROM. Many people find this more readable. Let's see this in action."
   ],
   "id": "5892cb8ab2e4f53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "db.sql(\"FROM services SELECT COUNT(*)\")",
   "id": "6619f7d5ed235fae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Another friendly SQL extension is [DESCRIBE](https://duckdb.org/docs/stable/guides/meta/describe.html), which provides a summary of table columns, data types, and their basic features. It's very useful to verify the data after loading CSV or other unreliable formats. It even allows skipping SELECT and FROM altogether.",
   "id": "2142aaa94612c94f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "db.sql(\"DESCRIBE services\")",
   "id": "5089e6472619ea30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's see a few examples of queries on this table:",
   "id": "aa504d497058d084"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# number of all the services\n",
    "db.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT \"Service:RDT-ID\") \n",
    "FROM services\n",
    "\"\"\")"
   ],
   "id": "ea9c7ef5083d057b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# number of stops per service\n",
    "db.sql(\"\"\"\n",
    "SELECT \"Service:RDT-ID\",\n",
    "       COUNT(*) AS num_stops\n",
    "FROM services\n",
    "GROUP BY \"Service:RDT-ID\"\n",
    "ORDER BY num_stops DESC\n",
    "\"\"\"\n",
    ")"
   ],
   "id": "d34d1bd37be6e314"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# average arrival delay per service type\n",
    "db.sql(\"\"\"\n",
    "SELECT \"Service:Type\",\n",
    "       AVG(\"Stop:Arrival delay\") AS avg_arrival_delay\n",
    "FROM services\n",
    "WHERE \"Stop:Arrival delay\" IS NOT NULL\n",
    "GROUP BY \"Service:Type\"\n",
    "ORDER BY avg_arrival_delay DESC\n",
    "\"\"\")"
   ],
   "id": "e53897888743d1a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Exercises**\n",
    "\n",
    "1. Which 10 stations were busiest, i.e. got the most train arrivals or departures?\n",
    "2. Load file `data/disruptions-2023.csv` as `disruptions` table. Describe it.\n",
    "3. What were the 5 most frequent disruption causes in 2023, and how many times did they occur?\n",
    "4. Select the row with the longest disruption. What was the cause? How long was it in days?"
   ],
   "id": "ab9a869805e1be45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9f083e3c6bf9d808"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Connecting to databases",
   "id": "f6762bed38d8c7bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "DuckDB can connect to various data sources, e.g. Excel, databases, cloud files, data lakehouses, and others. It uses a system of [extensions](https://duckdb.org/docs/stable/extensions/overview), which are loaded explicitly. This avoids bloating the base package and is very flexible. We will now install and load the Postgres extension, and query the database.\n",
    "\n",
    "Make sure that PostgreSQL is up and running in Docker Compose."
   ],
   "id": "b7eba5829383d837"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# install and import extension\n",
    "db.sql(\"\"\"INSTALL postgres; LOAD postgres;\"\"\")\n",
    "\n",
    "# NEVER write password like this explicitly in real code!\n",
    "conn_string = \"host=localhost user=postgres password=postgres dbname=postgres\"\n",
    "\n",
    "# note that we create the alias for this Postgres database here\n",
    "# this also needs to be in single quotes\n",
    "db.sql(f\"\"\"\n",
    "ATTACH IF NOT EXISTS '{conn_string}' AS postgres_db_read (TYPE postgres, READ_ONLY);\n",
    "\"\"\")"
   ],
   "id": "5a8cb87116a63ae1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# command to list all tables available\n",
    "db.sql(\"SHOW ALL TABLES\")"
   ],
   "id": "ba5a7dc6eead8860"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You can securely store connections information in [DuckDB secrets](https://duckdb.org/docs/stable/core_extensions/postgres#configuring-via-secrets). Here, we make a read-only connection, but DuckDB can also [write to Postgres](https://duckdb.org/docs/stable/core_extensions/postgres#writing-data-to-postgresql). This way you can e.g. insert Parquet data into Postgres, which is not supported natively.\n",
    "\n",
    "When a database is attached, the data is queried from it each time. It guarantees the newest data, but also results in networking overhead. You can [copy Postgres table](https://duckdb.org/docs/stable/core_extensions/postgres#usage) into DuckDB to avoid this, but you have stale data this way.\n",
    "\n",
    "We have two `services` tables now - to query the Postgres one, we prefix it with database alias. Compare querying Postgres and DuckDB tables."
   ],
   "id": "d43566a0176fed07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "db.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM postgres_db_read.services\n",
    "\"\"\")"
   ],
   "id": "dca32fa882592aca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "db.sql(\"\"\"\n",
    "SELECT COUNT(*)\n",
    "FROM services\n",
    "\"\"\")"
   ],
   "id": "79cb1749e041e19"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You can run any queries this way, e.g. JOIN files and databases, insert and update data sources, and more. To remove a database, use DETACH command:",
   "id": "443b225d7042bad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "db.sql(\"DETACH postgres_db_read\")",
   "id": "4d7e115c39252956"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Exercise**\n",
    "\n",
    "1. Download data about disruptions from 2024 ([Rijden de Treinen page](https://www.rijdendetreinen.nl/en/open-data/disruptions)). You can use either Python (e.g. [urllib](https://stackoverflow.com/a/19602990/9472066)) or wget.\n",
    "2. Create read-write connection to Postgres.\n",
    "3. Use [Postgres CREATE TABLE](https://duckdb.org/docs/stable/core_extensions/postgres#create-table) in DuckDB to insert both 2023 and 2024 disruptions data into Postgres database as `disruptions` table. See [documentation about reading multiple files](https://duckdb.org/docs/stable/data/multiple_files/overview.html).\n",
    "4. Query how many disruptions occurred in 2023 and 2024."
   ],
   "id": "87a83a19a5782001"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "384c94f459ef0f31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Writing data",
   "id": "304f880cf20ee7a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "DuckDB can not only insert data into databases, but also write regular files to disk, for example [JSON and JSON Lines](https://duckdb.org/docs/stable/data/json/writing_json) and [Parquet](https://duckdb.org/docs/stable/data/parquet/overview). It also makes it very convenient as a format conversion tool, e.g. as a first part in a data processing workflow.\n",
    "\n",
    "DuckDB is very fast at reading CSV, and [gets faster over time](https://duckdb.org/2024/06/26/benchmarks-over-time.html#csv-reader). However, its unique feature is [very robust and advanced CSV parsing](https://duckdb.org/2023/10/27/csv-sniffer.html), based on paper [\"Multiple hypothesis CSV parsing\" T. Döhmen et al.](https://hannes.muehleisen.org/publications/ssdbm2017-muehleisen-csvs.pdf), and in case of errors it has [verbose error informations](https://duckdb.org/docs/stable/data/csv/reading_faulty_csv_files), as well as option to omit faulty lines.\n",
    "\n",
    "Let's read the CSV file with 2024 train services and change it to TSV (tab-separated values) file. Exporting files also uses COPY command, with format-specific options ([documentation](https://duckdb.org/docs/stable/sql/statements/copy.html#csv-options))."
   ],
   "id": "53374a64fc29e9a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "duckdb.sql(\"\"\"\n",
    "COPY (SELECT * FROM \"data/services-2024.csv\") TO \"data/services_2024_duckdb.tsv\" (HEADER, DELIMITER \"\\t\");\n",
    "\"\"\")\n",
    "\n",
    "with open(\"data/services_2024_duckdb.tsv\") as file:\n",
    "    i = 0\n",
    "    for i, line in enumerate(file):\n",
    "        i += 1\n",
    "        print(line)\n",
    "        if i == 5:\n",
    "            break"
   ],
   "id": "99fb1942351a49d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Exercise**\n",
    "\n",
    "1. Convert the 2024 services file into JSON, JSON Lines and Parquet files. Use ZSTD compression for Parquet. Use [DuckDB documentation](https://duckdb.org/docs/stable/sql/statements/copy.html#format-specific-options).\n",
    "2. Compare file size of each file format (including CSV) in MB.\n",
    "3. Compare loading time of each file format (including CSV), e.g. to count how many trains arrived at Rotterdam Centraal station. Use `time` module. Report mean time of 3 repetitions."
   ],
   "id": "94c67b291d788cee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "272e9cc645521f9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DuckDB interoperability",
   "id": "b69c921237b858ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "DuckDB is highly interoperable with Python and with data processing frameworks like Pandas and Polars. It can convert results to them, as well as query DataFrames with SQL. This has a few uses:\n",
    "- switch to SQL whenever you want\n",
    "- use DuckDB for fast and robust data reading, e.g. for CSV\n",
    "- convert to Pandas for plotting the results\n",
    "- pool data from various sources with DuckDB and then build complex workflows with DataFrames\n",
    "\n",
    "Getting results in Python works similarly to `psycopg`, with methods `.fetchone()` and `.fetchall()`.\n",
    "\n",
    "Let's see how many services were run in January and December 2024."
   ],
   "id": "efacc68e06fbbbd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_jan_services = duckdb.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT \"Service:RDT-ID\")\n",
    "FROM \"data/services-2024.csv\"\n",
    "WHERE \"Service:Date\" BETWEEN '2024-01-01' AND '2024-01-31'\n",
    "\"\"\")\n",
    "num_jan_services = num_jan_services.fetchone()[0]\n",
    "\n",
    "num_dec_services = duckdb.sql(\"\"\"\n",
    "SELECT COUNT(DISTINCT \"Service:RDT-ID\")\n",
    "FROM \"data/services-2024.csv\"\n",
    "WHERE \"Service:Date\" BETWEEN '2024-12-01' AND '2024-12-31'\n",
    "\"\"\")\n",
    "num_dec_services = num_dec_services.fetchone()[0]\n",
    "\n",
    "print(f\"January services: {num_jan_services}\")\n",
    "print(f\"December services: {num_dec_services}\")"
   ],
   "id": "a77081df9830e7dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Converting to Pandas is similar, with method `.to_df()` (aliases: `.fetch_df()`, `.df()`). Let's fetch all services by month.",
   "id": "d5e2a00121168c0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_monthly_services = duckdb.sql(\"\"\"\n",
    "SELECT MONTH(\"Service:Date\") AS month, COUNT(DISTINCT \"Service:RDT-ID\") AS num_services \n",
    "FROM \"data/services-2024.csv\"\n",
    "GROUP BY MONTH(\"Service:Date\")\n",
    "ORDER BY month\n",
    "\"\"\").df()\n",
    "\n",
    "num_monthly_services"
   ],
   "id": "ef311b64ba32d06e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since we're already here, let's visualize this with Matplotlib. Pandas has handy shortcuts in `.plot` method ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html)).",
   "id": "8538855cf0c1d9e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_monthly_services.plot.line(\n",
    "    x=\"month\", y=\"num_services\", title=\"Number of services in 2024 by month\"\n",
    ")"
   ],
   "id": "81b7494fe2b4509"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can also query Pandas DataFrames by just using variable names like tables. Note that we should use Arrow backend for good performance here. Let's load the disruptions from 2023 and check for frequency of top 10 most popular causes. Then we will go back to Pandas and make a bar plot.",
   "id": "6ac6acf04075539"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_disruptions = pd.read_csv(\"data/disruptions-2023.csv\", dtype_backend=\"pyarrow\")\n",
    "\n",
    "df_disruptions_statistics = duckdb.sql(\"\"\"\n",
    "SELECT statistical_cause_en AS cause, COUNT(*) / (SELECT COUNT(*) FROM df_disruptions) AS cause_frequency\n",
    "FROM df_disruptions\n",
    "GROUP BY statistical_cause_en\n",
    "ORDER BY cause_frequency DESC\n",
    "LIMIT 10\n",
    "\"\"\").df()\n",
    "\n",
    "df_disruptions_statistics.plot.barh(x=\"cause\", y=\"cause_frequency\")"
   ],
   "id": "ff8766ca904f6903"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Exercise**\n",
    "\n",
    "1. Extract the average daily arrival delays of 2024 train services with DuckDB. Note that NULL delay means zero, and you have to use [COALESCE function](https://duckdb.org/docs/stable/sql/functions/utility.html#coalesceexpr-) to fill them as zeros.\n",
    "2. Visualize those changes over time with Pandas on a line plot. Plot daily average, and also rolling mean (average) of 7 and 30 days to analyze longer trends."
   ],
   "id": "d539c2c7b2b5987e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c55f0388b19817b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "212b310faaaaad63"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
